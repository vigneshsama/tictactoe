{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b7f9ce-0503-47f3-b8ae-50e04f1263d6",
   "metadata": {},
   "source": [
    "Tic-Tac-Toe game code utilizes Q-learning, a reinforcement learning algorithm, to train an AI agent to play optimally against a human player. The game environment is encapsulated in the TicTacToe class, which manages the state of the board, checks for available actions, and determines if a player has won or if there is a tie. The QLearningAgent class implements the Q-learning algorithm, where the agent learns to select the best moves based on the current state of the game. It maintains a Q-table to store the values for each state-action pair, which it updates using the Bellman equation after each move. The agent chooses its moves based on either exploration (random move) or exploitation (best-known move). The game proceeds with the AI making its move, followed by the player's move, and updates the board after each action. The draw_board function is responsible for displaying the board in a clean, user-friendly format. The AI is trained using 1,000 episodes of gameplay, learning from its experiences before playing against the human player. The game continues until either the player or the AI wins, or a tie occurs. The main flow of the game includes interactions between the AI, which makes moves based on the learned strategy, and the human player, who inputs their move manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc793b2-a0f4-4232-af83-d5793e8af61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Tic-Tac-Toe!\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n",
      "AI chooses move 1\n",
      "  | X |  \n",
      "---------\n",
      "  |   |  \n",
      "---------\n",
      "  |   |  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-8):  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | X |  \n",
      "---------\n",
      "  | O |  \n",
      "---------\n",
      "  |   |  \n",
      "AI chooses move 8\n",
      "  | X |  \n",
      "---------\n",
      "  | O |  \n",
      "---------\n",
      "  |   | X\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-8):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | X |  \n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "  |   | X\n",
      "AI chooses move 0\n",
      "X | X |  \n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "  |   | X\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-8):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X | X | O\n",
      "---------\n",
      "O | O |  \n",
      "---------\n",
      "  |   | X\n",
      "AI chooses move 5\n",
      "X | X | O\n",
      "---------\n",
      "O | O | X\n",
      "---------\n",
      "  |   | X\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-8):  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X | X | O\n",
      "---------\n",
      "O | O | X\n",
      "---------\n",
      "O |   | X\n",
      "Game Over!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define the Tic-Tac-Toe environment\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = [\" \" for _ in range(9)]  # Empty board\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = [\" \" for _ in range(9)]\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.board\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [i for i, x in enumerate(self.board) if x == \" \"]\n",
    "\n",
    "    def take_action(self, action, player):\n",
    "        if self.board[action] == \" \":\n",
    "            self.board[action] = player\n",
    "            if self.check_winner(player):\n",
    "                self.done = True\n",
    "                self.winner = player\n",
    "            elif \" \" not in self.board:\n",
    "                self.done = True\n",
    "                self.winner = \"Tie\"\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_winner(self, player):\n",
    "        win_positions = [\n",
    "            (0, 1, 2), (3, 4, 5), (6, 7, 8),  # Rows\n",
    "            (0, 3, 6), (1, 4, 7), (2, 5, 8),  # Columns\n",
    "            (0, 4, 8), (2, 4, 6)              # Diagonals\n",
    "        ]\n",
    "        for a, b, c in win_positions:\n",
    "            if self.board[a] == self.board[b] == self.board[c] == player:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_reward(self):\n",
    "        if self.winner == \"X\":\n",
    "            return 1  # AI wins\n",
    "        elif self.winner == \"O\":\n",
    "            return -1  # Player wins\n",
    "        elif self.winner == \"Tie\":\n",
    "            return 0  # Tie\n",
    "        return 0\n",
    "\n",
    "# Define the Q-Learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, epsilon=0.1, alpha=0.5, gamma=0.9):\n",
    "        self.q_table = {}  # Q-values will be stored here\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        if (tuple(state), action) not in self.q_table:\n",
    "            self.q_table[(tuple(state), action)] = 0  # Initialize if not seen before\n",
    "        return self.q_table[(tuple(state), action)]\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state, next_action):\n",
    "        max_q_value = max([self.get_q_value(next_state, a) for a in range(9)])\n",
    "        current_q_value = self.get_q_value(state, action)\n",
    "        new_q_value = current_q_value + self.alpha * (reward + self.gamma * max_q_value - current_q_value)\n",
    "        self.q_table[(tuple(state), action)] = new_q_value\n",
    "\n",
    "    def choose_action(self, state, available_actions):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(available_actions)  # Explore\n",
    "        else:\n",
    "            q_values = [self.get_q_value(state, a) for a in available_actions]\n",
    "            max_q_value = max(q_values)\n",
    "            best_actions = [a for a in available_actions if self.get_q_value(state, a) == max_q_value]\n",
    "            return random.choice(best_actions)  # Exploit\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        for _ in range(episodes):\n",
    "            env = TicTacToe()\n",
    "            state = env.reset()\n",
    "            while not env.done:\n",
    "                available_actions = env.available_actions()\n",
    "                action = self.choose_action(state, available_actions)\n",
    "                env.take_action(action, \"X\")\n",
    "                reward = env.get_reward()\n",
    "                next_state = env.board\n",
    "                if env.done:\n",
    "                    self.update_q_value(state, action, reward, next_state, None)\n",
    "                    break\n",
    "                next_action = self.choose_action(next_state, available_actions)\n",
    "                self.update_q_value(state, action, reward, next_state, next_action)\n",
    "                state = next_state\n",
    "\n",
    "# Print the board in the requested format\n",
    "def draw_board(board):\n",
    "    print(f\"{board[0]} | {board[1]} | {board[2]}\")\n",
    "    print(\"---------\")\n",
    "    print(f\"{board[3]} | {board[4]} | {board[5]}\")\n",
    "    print(\"---------\")\n",
    "    print(f\"{board[6]} | {board[7]} | {board[8]}\")\n",
    "\n",
    "# Test the Q-Learning agent\n",
    "if __name__ == \"__main__\":\n",
    "    agent = QLearningAgent()\n",
    "    agent.train(1000)  # Train the agent\n",
    "\n",
    "    # Play a game\n",
    "    env = TicTacToe()\n",
    "    state = env.reset()\n",
    "    print(\"Welcome to Tic-Tac-Toe!\")\n",
    "    draw_board(state)\n",
    "\n",
    "    while not env.done:\n",
    "        available_actions = env.available_actions()\n",
    "        action = agent.choose_action(state, available_actions)\n",
    "        print(f\"AI chooses move {action}\")\n",
    "        env.take_action(action, \"X\")\n",
    "        draw_board(env.board)\n",
    "        if env.done:\n",
    "            break\n",
    "\n",
    "        # Player move\n",
    "        player_move = int(input(\"Enter your move (0-8): \"))\n",
    "        env.take_action(player_move, \"O\")\n",
    "        draw_board(env.board)\n",
    "\n",
    "    print(\"Game Over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708eee04-f9e4-4294-9c77-b476145f8550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
